{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\n",
       "// Add Delta Lake dependency\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\n",
       "// Add Spark SQL dependency\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\n",
       "//hadoop hdfs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scala-lang:scala-library:2.13.12`\n",
    "\n",
    "// Add Delta Lake dependency\n",
    "import $ivy.`io.delta:delta-spark_2.13:3.1.0`\n",
    "\n",
    "// Add Spark SQL dependency\n",
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import $ivy.`org.apache.spark:spark-core_2.13:3.5.1`\n",
    "\n",
    "//hadoop hdfs\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:3.3.6`\n",
    "import $ivy.`org.apache.hadoop:hadoop-client:3.3.6`\n",
    "import $ivy.`org.apache.hadoop:hadoop-client-api:3.3.6`\n",
    "import $ivy.`org.apache.hadoop:hadoop-hdfs:3.3.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io.delta.tables.DeltaTable\n",
    "import org.apache.spark.sql.{Encoders, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark=SparkSession.builder()\n",
    "    .appName(\"Fetch Delta Table\")\n",
    "    .master(\"local[*]\")//runs on localhost 4040\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\")//setup for connecting to hadoop, comment it out if not required\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.autoCompact.enabled\", \"true\") // Enable auto-compaction globally. By default will use 128 MB as the target file size.\n",
    "    .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "    .config(\"spark.databricks.delta.autoCompact.targetFileSize\", \"256MB\") // Set target file size for auto-compaction\n",
    "    .config(\"spark.databricks.delta.optimizeWrite.binSize\", \"256MB\") // Set bin size for optimized writes\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val delPath=\"hdfs://localhost:9000/delta/dataF\"\n",
    "val delPath2=\"hdfs://localhost:9000/delta/dataFsms\"\n",
    "val delPath3=\"hdfs://localhost:9000/delta/dataFesewa\"\n",
    "val delPath4=\"/home/avyuthan-shah/Desktop/Data/dataF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Z-Order</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile(\"true\")//to monitor load\n",
    "val startTime = System.nanoTime()\n",
    "val dt=DeltaTable.forPath(spark, delPath)\n",
    "dt.optimize().executeZOrderBy(\"AccountNo\")\n",
    "val endTime = System.nanoTime()\n",
    "writeFile(\"false\")\n",
    "val elapsedTime = (endTime - startTime) / 1e9 // Time in seconds\n",
    "println(s\"Elapsed time for Zordering: '$elapsedTime' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val version=DeltaTable.forPath(spark, delPath).history(1).select(\"version\").as[Long](Encoders.scalaLong).head //Get Latest Version\n",
    "writeFile(\"true\")//to monitor load\n",
    "val startTime = System.nanoTime()\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\",version).load(delPath).createOrReplaceTempView(\"delta_table\")\n",
    "\n",
    "val result = spark.sql(\n",
    "   s\"\"\"\n",
    "   |SELECT * FROM delta_table\n",
    "   |WHERE AccountNo IN (\"56135\",\"69310\",\"65260\")\n",
    "   |ORDER BY AccountNo,VALUEDATE DESC,TIME DESC;\n",
    "   \"\"\".stripMargin)\n",
    "    \n",
    "result.show(truncate=false)\n",
    "val endTime = System.nanoTime()\n",
    "writeFile(\"false\")\n",
    "val elapsedTime = (endTime - startTime) / 1e9 // Time in seconds\n",
    "println(s\"Elapsed time for retrival: '$elapsedTime' \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
